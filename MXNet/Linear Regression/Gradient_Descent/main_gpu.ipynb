{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### Linear Regression with Gradient Descent \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "The formulation of the loss function is given as-\n",
    "Formula: $$ L = \\frac{1}{2n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$\n",
    "The gradient of the loss function is given as-\n",
    "Formula: $$ \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true}) \\cdot x $$\n",
    "The weights are updated as-\n",
    "Formula: $$ w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages and libraries\n",
    "from mxnet import nd, gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctx = mx.gpu()\n",
    "model_ctx = mx.gpu()\n",
    "# load the dataset\n",
    "data = pd.read_csv('custom_2017_2020.csv')\n",
    "# convert to numpy array\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data = (data - mean) / std\n",
    "# convert to NDArray\n",
    "data = nd.array(data, ctx=data_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into features and labels\n",
    "features = nd.array(data[:, :-1], ctx=data_ctx)\n",
    "labels = nd.array(data[:, -1], ctx=data_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features\n",
    "y_train = labels\n",
    "y_train = y_train.reshape((len(y_train), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14897200, 12)\n",
      "(14897200, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the training set to check dimensions\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring the parameters and important variables\n",
    "no_of_data_points = X_train.shape[0]\n",
    "no_of_features = X_train.shape[1]\n",
    "no_of_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights initialisation\n",
    "weights = np.random.normal(0, 1, (no_of_features, 1))\n",
    "# bias initialisation\n",
    "bias = np.random.normal(0, 1, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyperparameters\n",
    "learning_rate = 0.0001\n",
    "derivative_weights = np.zeros((no_of_features, 1))\n",
    "derivative_bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to NDArray\n",
    "weights = nd.array(weights, ctx=model_ctx)\n",
    "bias = nd.array(bias, ctx=model_ctx)\n",
    "derivative_weights = nd.array(derivative_weights, ctx=model_ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(no_of_epochs):\n",
    "    y_pred = nd.dot(X_train, weights) + bias\n",
    "    difference = y_train - y_pred\n",
    "    derivative_weights = nd.dot(X_train.T, difference) / no_of_data_points\n",
    "    derivative_bias = nd.sum(difference) / no_of_data_points\n",
    "    # updating the weights and bias\n",
    "    weights = weights + learning_rate * derivative_weights\n",
    "    bias = bias + learning_rate * derivative_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model using gradient: 1.3633968830108643 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken to train the model using gradient descent: {end - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
