{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "1. Ordinary Least Squares Method: \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "\n",
    "Formula:  $$ weights = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$$\n",
    "\n",
    "To find the predicted values, we multiply the feature matrix X with the weights vector.\n",
    "Formula: $$ y_{pred} = X \\cdot weights $$\n",
    "\n",
    "Mean Squared Error: $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import ndarray as nd\n",
    "from LinearRegression import LinearRegression\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from DAT file in NDArray format\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()\n",
    "data_file = open(\"airfoil_self_noise.dat\", \"r\")\n",
    "data = np.loadtxt(data_file, delimiter=\"\\t\")\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and labels\n",
    "features = nd.array(data[:, 0:-1], ctx=data_ctx)\n",
    "labels = nd.array(data[:, -1], ctx=data_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing sets (80% training, 20% testing)\n",
    "X_train = features[:int(len(features)*0.8)]\n",
    "X_test = features[int(len(features)*0.8):]\n",
    "y_train = labels[:int(len(labels)*0.8)]\n",
    "y_test = labels[int(len(labels)*0.8):]\n",
    "y_train = y_train.reshape((len(y_train), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1202, 5)\n",
      "(301, 5)\n",
      "(1202, 1)\n",
      "(301, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the training and testing sets to check dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using OLS Method\n",
    "linear_regression = LinearRegression()\n",
    "# note down the time before training\n",
    "start = time.time()\n",
    "weights = linear_regression.OLS_fit(X_train, y_train)\n",
    "# note down the time after training\n",
    "end = time.time()\n",
    "y_pred = linear_regression.OLS_predict(X_test, weights)\n",
    "mse = nd.mean(nd.square(y_test - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  26.421574\n"
     ]
    }
   ],
   "source": [
    "# printing the result of the OLS Method\n",
    "print(\"MSE: \", mse.asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the Linear regression model (using normal method) using CPU: 0.0009999275207519531 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken to train the Linear regression model (using normal method) using CPU: {end - start} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### Linear Regression with Gradient Descent \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "The formulation of the loss function is given as-\n",
    "Formula: $$ L = \\frac{1}{2n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$\n",
    "The gradient of the loss function is given as-\n",
    "Formula: $$ \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true}) \\cdot x $$\n",
    "The weights are updated as-\n",
    "Formula: $$ w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using Gradient Descent Method with MXNet\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import np, npx\n",
    "import mxnet as mx\n",
    "npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from DAT file in NDArray format\n",
    "data = np.genfromtxt('airfoil_self_noise.dat', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into features and labels\n",
    "\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "labels = np.reshape(labels, (labels.shape[0], 1))\n",
    "# convert the data into float32 format\n",
    "features = features.astype(np.float32)\n",
    "labels = labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data set\n",
    "\n",
    "X_train = features[:features.shape[0]*8//10, :]\n",
    "X_test = features[features.shape[0]*8//10:, :]\n",
    "y_train = labels[:features.shape[0]*8//10, :]\n",
    "y_test = labels[features.shape[0]*8//10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the gradient descent \n",
    "# declaring the parameters and important variables\n",
    "no_of_data_points = X_train.shape[0]\n",
    "no_of_features = X_train.shape[1]\n",
    "no_of_epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights initialisation\n",
    "weights = np.random.normal(0, 1, (no_of_features, 1))\n",
    "# bias initialisation\n",
    "bias = np.random.normal(0, 1, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for the gradient descent method\n",
    "learning_rate = 0.00000000001\n",
    "num_of_epochs = 100000\n",
    "derivative_weights = np.zeros((no_of_features, 1))\n",
    "derivative_intercept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_of_epochs):\n",
    "    # calculating the derivative of the weights and intercept\n",
    "    y_pred = np.dot(X_train, weights) + bias\n",
    "    difference = y_pred - y_train\n",
    "    derivative_weights = np.dot(X_train.T, difference)\n",
    "    derivative_intercept = np.sum(difference)\n",
    "    # updating the weights and intercept\n",
    "    weights = weights - learning_rate * derivative_weights\n",
    "    bias = bias - learning_rate * derivative_intercept\n",
    "    if (epoch%10000==0): \n",
    "        MSE = np.mean(np.square(y_train - y_pred))\n",
    "        print(f\"Epoch No.: {epoch}, MSE: {MSE}, Absolute Error: {np.mean(np.abs(y_train - y_pred))}\")\n",
    "        if (MSE < 0.0001):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the MSE for the testing data\n",
    "y_pred = np.dot(X_test, weights) + bias\n",
    "MSE = np.mean(np.square(y_test - y_pred))\n",
    "print(\"MSE: \", MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Stochastic Gradient Descent Optimization (SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using \n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import np, npx\n",
    "import mxnet as mx\n",
    "npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from DAT file in NDArray format\n",
    "data = np.genfromtxt('airfoil_self_noise.dat', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into features and labels\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "labels = np.reshape(labels, (labels.shape[0], 1))\n",
    "# convert the data into float32 format\n",
    "features = features.astype(np.float32)\n",
    "labels = labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data set\n",
    "\n",
    "X_train = features[:features.shape[0]*8//10, :]\n",
    "y_train = labels[:features.shape[0]*8//10, :]\n",
    "X_test = features[features.shape[0]*8//10:, :]\n",
    "y_test = labels[features.shape[0]*8//10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data iterator\n",
    "batch_size = 10\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "neural_network = gluon.nn.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the weights and bias\n",
    "neural_network.collect_params().initialize(mx.init.Normal(sigma=0.000000001), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "loss_function = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizer\n",
    "optimizer = gluon.Trainer(neural_network.collect_params(), 'sgd', {'learning_rate': 0.00000000001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "loss_sequence = []\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = neural_network(data)\n",
    "            loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step(batch_size)\n",
    "        cumulative_loss += loss.mean()\n",
    "    if epoch%10==0:\n",
    "        print(f\"Epoch {epoch}, loss: {cumulative_loss/len(train_data)}\")\n",
    "    loss_sequence.append(cumulative_loss)\n",
    "    # checking for the stopping criterion\n",
    "    # if the MSE is less than 0.01, then break out of the loop\n",
    "    if (cumulative_loss/len(train_data) < 0.01):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the MSE for the testing data\n",
    "cumulative_loss = 0\n",
    "for i, (data, label) in enumerate(test_data):\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    label = label.as_in_context(model_ctx)\n",
    "    output = neural_network(data)\n",
    "    loss = loss_function(output, label)\n",
    "    cumulative_loss += loss.mean()\n",
    "print(f\"Testing loss: {cumulative_loss/len(test_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
