{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "1. Ordinary Least Squares Method: \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "\n",
    "Formula:  $$ weights = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$$\n",
    "\n",
    "To find the predicted values, we multiply the feature matrix X with the weights vector.\n",
    "Formula: $$ y_{pred} = X \\cdot weights $$\n",
    "\n",
    "Mean Squared Error: $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import ndarray as nd\n",
    "from LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from DAT file in NDArray format\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()\n",
    "data_file = open(\"airfoil_self_noise.dat\", \"r\")\n",
    "data = np.loadtxt(data_file, delimiter=\"\\t\")\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and labels\n",
    "features = nd.array(data[:, 0:-1], ctx=data_ctx)\n",
    "labels = nd.array(data[:, -1], ctx=data_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing sets (80% training, 20% testing)\n",
    "X_train = features[:int(len(features)*0.8)]\n",
    "X_test = features[int(len(features)*0.8):]\n",
    "y_train = labels[:int(len(labels)*0.8)]\n",
    "y_test = labels[int(len(labels)*0.8):]\n",
    "y_train = y_train.reshape((len(y_train), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1202, 5)\n",
      "(301, 5)\n",
      "(1202, 1)\n",
      "(301, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the training and testing sets to check dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using OLS Method\n",
    "linear_regression = LinearRegression()\n",
    "weights = linear_regression.OLS_fit(X_train, y_train)\n",
    "y_pred = linear_regression.OLS_predict(X_test, weights)\n",
    "mse = nd.mean(nd.square(y_test - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  26.421574\n"
     ]
    }
   ],
   "source": [
    "# printing the result of the OLS Method\n",
    "print(\"MSE: \", mse.asscalar())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### Linear Regression with Gradient Descent \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "The formulation of the loss function is given as-\n",
    "Formula: $$ L = \\frac{1}{2n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$\n",
    "The gradient of the loss function is given as-\n",
    "Formula: $$ \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true}) \\cdot x $$\n",
    "The weights are updated as-\n",
    "Formula: $$ w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using Gradient Descent Method with MXNet\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, nd\n",
    "\n",
    "# defining the hyperparameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 10\n",
    "\n",
    "# defining the model\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.initialize(mx.init.Normal(sigma=0.01), ctx=model_ctx)\n",
    "\n",
    "# defining the loss function\n",
    "square_loss = mx.gluon.loss.L2Loss()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
