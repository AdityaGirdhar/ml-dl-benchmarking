{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "1. Ordinary Least Squares Method: \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "\n",
    "Formula:  $$ weights = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$$\n",
    "\n",
    "To find the predicted values, we multiply the feature matrix X with the weights vector.\n",
    "Formula: $$ y_{pred} = X \\cdot weights $$\n",
    "\n",
    "Mean Squared Error: $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import ndarray as nd\n",
    "from LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from DAT file in NDArray format\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()\n",
    "data_file = open(\"airfoil_self_noise.dat\", \"r\")\n",
    "data = np.loadtxt(data_file, delimiter=\"\\t\")\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and labels\n",
    "features = nd.array(data[:, 0:-1], ctx=data_ctx)\n",
    "labels = nd.array(data[:, -1], ctx=data_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing sets (80% training, 20% testing)\n",
    "X_train = features[:int(len(features)*0.8)]\n",
    "X_test = features[int(len(features)*0.8):]\n",
    "y_train = labels[:int(len(labels)*0.8)]\n",
    "y_test = labels[int(len(labels)*0.8):]\n",
    "y_train = y_train.reshape((len(y_train), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1202, 5)\n",
      "(301, 5)\n",
      "(1202, 1)\n",
      "(301, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the training and testing sets to check dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prakhar Gupta\\Documents\\GitHub\\dummy_ip\\Linear Regression\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# implementing linear regression using OLS Method\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m linear_regression \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m weights \u001b[39m=\u001b[39m linear_regression\u001b[39m.\u001b[39;49mOLS_fit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y_pred \u001b[39m=\u001b[39m linear_regression\u001b[39m.\u001b[39mOLS_predict(X_test, weights)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mse \u001b[39m=\u001b[39m nd\u001b[39m.\u001b[39mmean(nd\u001b[39m.\u001b[39msquare(y_test \u001b[39m-\u001b[39m y_pred))\n",
      "File \u001b[1;32mc:\\Users\\Prakhar Gupta\\Documents\\GitHub\\dummy_ip\\Linear Regression\\LinearRegression.py:13\u001b[0m, in \u001b[0;36mLinearRegression.OLS_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mOLS_fit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m     12\u001b[0m     \u001b[39m# adding a column of ones to X\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     X \u001b[39m=\u001b[39m nd\u001b[39m.\u001b[39mconcat(X, nd\u001b[39m.\u001b[39mones((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[39m# calculating the weights\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     product \u001b[39m=\u001b[39m nd\u001b[39m.\u001b[39mlinalg_gemm2(X, X, transpose_a\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nd' is not defined"
     ]
    }
   ],
   "source": [
    "# implementing linear regression using OLS Method\n",
    "linear_regression = LinearRegression()\n",
    "weights = linear_regression.OLS_fit(X_train, y_train)\n",
    "y_pred = linear_regression.OLS_predict(X_test, weights)\n",
    "mse = nd.mean(nd.square(y_test - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  26.421574\n"
     ]
    }
   ],
   "source": [
    "# printing the result of the OLS Method\n",
    "print(\"MSE: \", mse.asscalar())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### Linear Regression with Gradient Descent \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "The formulation of the loss function is given as-\n",
    "Formula: $$ L = \\frac{1}{2n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$\n",
    "The gradient of the loss function is given as-\n",
    "Formula: $$ \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true}) \\cdot x $$\n",
    "The weights are updated as-\n",
    "Formula: $$ w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using Gradient Descent Method with MXNet\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import np, npx\n",
    "import mxnet as mx\n",
    "npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from DAT file in NDArray format\n",
    "data = np.genfromtxt('airfoil_self_noise.dat', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into features and labels\n",
    "\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "labels = np.reshape(labels, (labels.shape[0], 1))\n",
    "# convert the data into float32 format\n",
    "features = features.astype(np.float32)\n",
    "labels = labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data set\n",
    "\n",
    "X_train = features[:features.shape[0]*8//10, :]\n",
    "X_test = features[features.shape[0]*8//10:, :]\n",
    "y_train = labels[:features.shape[0]*8//10, :]\n",
    "y_test = labels[features.shape[0]*8//10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the gradient descent \n",
    "# declaring the parameters and important variables\n",
    "no_of_data_points = X_train.shape[0]\n",
    "no_of_features = X_train.shape[1]\n",
    "no_of_epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights initialisation\n",
    "weights = np.random.normal(0, 1, (no_of_features, 1))\n",
    "# bias initialisation\n",
    "bias = np.random.normal(0, 1, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for the gradient descent method\n",
    "learning_rate = 0.00000000001\n",
    "num_of_epochs = 100000\n",
    "derivative_weights = np.zeros((no_of_features, 1))\n",
    "derivative_intercept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No.: 0, MSE: 6448506.0, Absolute Error: 1793.491455078125\n",
      "Epoch No.: 10000, MSE: 10167.7890625, Absolute Error: 91.83922576904297\n",
      "Epoch No.: 20000, MSE: 7435.7685546875, Absolute Error: 78.46195983886719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Prakhar Gupta\\Documents\\GitHub\\dummy_ip\\Linear Regression\\main.ipynb Cell 17\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X_train, weights) \u001b[39m+\u001b[39m bias\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m difference \u001b[39m=\u001b[39m y_pred \u001b[39m-\u001b[39m y_train\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m derivative_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X_train\u001b[39m.\u001b[39;49mT, difference)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m derivative_intercept \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(difference)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prakhar%20Gupta/Documents/GitHub/dummy_ip/Linear%20Regression/main.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# updating the weights and intercept\u001b[39;00m\n",
      "File \u001b[1;32m<string>:51\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, out, name, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Prakhar Gupta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mxnet\\_ctypes\\ndarray.py:82\u001b[0m, in \u001b[0;36m_imperative_invoke\u001b[1;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m# return output stypes to avoid the c_api call for checking\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m# a handle's stype in _ndarray_cls\u001b[39;00m\n\u001b[0;32m     80\u001b[0m out_stypes \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mPOINTER(ctypes\u001b[39m.\u001b[39mc_int)()\n\u001b[1;32m---> 82\u001b[0m check_call(_LIB\u001b[39m.\u001b[39;49mMXImperativeInvokeEx(\n\u001b[0;32m     83\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_void_p(handle),\n\u001b[0;32m     84\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(\u001b[39mlen\u001b[39;49m(ndargs)),\n\u001b[0;32m     85\u001b[0m     c_handle_array(ndargs),\n\u001b[0;32m     86\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(num_output),\n\u001b[0;32m     87\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(output_vars),\n\u001b[0;32m     88\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(\u001b[39mlen\u001b[39;49m(keys)),\n\u001b[0;32m     89\u001b[0m     c_str_array(keys),\n\u001b[0;32m     90\u001b[0m     c_str_array([\u001b[39mstr\u001b[39;49m(s) \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m vals]),\n\u001b[0;32m     91\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_stypes)))\n\u001b[0;32m     93\u001b[0m create_ndarray_fn \u001b[39m=\u001b[39m _global_var\u001b[39m.\u001b[39m_np_ndarray_cls \u001b[39mif\u001b[39;00m is_np_op \u001b[39melse\u001b[39;00m _global_var\u001b[39m.\u001b[39m_ndarray_cls\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m original_output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_of_epochs):\n",
    "    # calculating the derivative of the weights and intercept\n",
    "    y_pred = np.dot(X_train, weights) + bias\n",
    "    difference = y_pred - y_train\n",
    "    derivative_weights = np.dot(X_train.T, difference)\n",
    "    derivative_intercept = np.sum(difference)\n",
    "    # updating the weights and intercept\n",
    "    weights = weights - learning_rate * derivative_weights\n",
    "    bias = bias - learning_rate * derivative_intercept\n",
    "    if (epoch%10000==0): \n",
    "        MSE = np.mean(np.square(y_train - y_pred))\n",
    "        print(f\"Epoch No.: {epoch}, MSE: {MSE}, Absolute Error: {np.mean(np.abs(y_train - y_pred))}\")\n",
    "        if (MSE < 0.0001):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1195.8896\n"
     ]
    }
   ],
   "source": [
    "# calculating the MSE for the testing data\n",
    "y_pred = np.dot(X_test, weights) + bias\n",
    "MSE = np.mean(np.square(y_test - y_pred))\n",
    "print(\"MSE: \", MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Stochastic Gradient Descent Optimization (SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using \n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import np, npx\n",
    "import mxnet as mx\n",
    "npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from DAT file in NDArray format\n",
    "data = np.genfromtxt('airfoil_self_noise.dat', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into features and labels\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "labels = np.reshape(labels, (labels.shape[0], 1))\n",
    "# convert the data into float32 format\n",
    "features = features.astype(np.float32)\n",
    "labels = labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data set\n",
    "\n",
    "X_train = features[:features.shape[0]*8//10, :]\n",
    "y_train = labels[:features.shape[0]*8//10, :]\n",
    "X_test = features[features.shape[0]*8//10:, :]\n",
    "y_test = labels[features.shape[0]*8//10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data iterator\n",
    "batch_size = 10\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "neural_network = gluon.nn.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the weights and bias\n",
    "neural_network.collect_params().initialize(mx.init.Normal(sigma=0.000000001), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "loss_function = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizer\n",
    "optimizer = gluon.Trainer(neural_network.collect_params(), 'sgd', {'learning_rate': 0.00000000001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 4442.61962890625\n",
      "Epoch 10, loss: 4416.0029296875\n",
      "Epoch 20, loss: 4417.1171875\n",
      "Epoch 30, loss: 4426.173828125\n",
      "Epoch 40, loss: 4440.517578125\n",
      "Epoch 50, loss: 4442.251953125\n",
      "Epoch 60, loss: 4434.6064453125\n",
      "Epoch 70, loss: 4426.00146484375\n",
      "Epoch 80, loss: 4414.99609375\n",
      "Epoch 90, loss: 4421.89599609375\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "loss_sequence = []\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = neural_network(data)\n",
    "            loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step(batch_size)\n",
    "        cumulative_loss += loss.mean()\n",
    "    if epoch%10==0:\n",
    "        print(f\"Epoch {epoch}, loss: {cumulative_loss/len(train_data)}\")\n",
    "    loss_sequence.append(cumulative_loss)\n",
    "    # checking for the stopping criterion\n",
    "    # if the MSE is less than 0.01, then break out of the loop\n",
    "    if (cumulative_loss/len(train_data) < 0.01):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 4495.39501953125\n"
     ]
    }
   ],
   "source": [
    "# Calculating the MSE for the testing data\n",
    "cumulative_loss = 0\n",
    "for i, (data, label) in enumerate(test_data):\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    label = label.as_in_context(model_ctx)\n",
    "    output = neural_network(data)\n",
    "    loss = loss_function(output, label)\n",
    "    cumulative_loss += loss.mean()\n",
    "print(f\"Testing loss: {cumulative_loss/len(test_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
