{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "1. Ordinary Least Squares Method: \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "\n",
    "Formula:  $$ weights = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$$\n",
    "\n",
    "To find the predicted values, we multiply the feature matrix X with the weights vector.\n",
    "Formula: $$ y_{pred} = X \\cdot weights $$\n",
    "\n",
    "Mean Squared Error: $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import ndarray as nd\n",
    "from LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from DAT file in NDArray format\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()\n",
    "data_file = open(\"airfoil_self_noise.dat\", \"r\")\n",
    "data = np.loadtxt(data_file, delimiter=\"\\t\")\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and labels\n",
    "features = nd.array(data[:, 0:-1], ctx=data_ctx)\n",
    "labels = nd.array(data[:, -1], ctx=data_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing sets (80% training, 20% testing)\n",
    "X_train = features[:int(len(features)*0.8)]\n",
    "X_test = features[int(len(features)*0.8):]\n",
    "y_train = labels[:int(len(labels)*0.8)]\n",
    "y_test = labels[int(len(labels)*0.8):]\n",
    "y_train = y_train.reshape((len(y_train), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1202, 5)\n",
      "(301, 5)\n",
      "(1202, 1)\n",
      "(301, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of the training and testing sets to check dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing linear regression using OLS Method\n",
    "linear_regression = LinearRegression()\n",
    "weights = linear_regression.OLS_fit(X_train, y_train)\n",
    "y_pred = linear_regression.OLS_predict(X_test, weights)\n",
    "mse = nd.mean(nd.square(y_test - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  26.421574\n"
     ]
    }
   ],
   "source": [
    "# printing the result of the OLS Method\n",
    "print(\"MSE: \", mse.asscalar())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### Linear Regression with Gradient Descent \n",
    "In this method, we find the regression coefficient weights that minimize the sum of the squared residuals.\n",
    "The formulation of the loss function is given as-\n",
    "Formula: $$ L = \\frac{1}{2n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2 $$\n",
    "The gradient of the loss function is given as-\n",
    "Formula: $$ \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true}) \\cdot x $$\n",
    "The weights are updated as-\n",
    "Formula: $$ w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # implementing linear regression using Gradient Descent Method with MXNet\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import np, npx\n",
    "import mxnet as mx\n",
    "npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from DAT file in NDArray format\n",
    "data = np.genfromtxt('airfoil_self_noise.dat', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into features and labels\n",
    "\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "labels = np.reshape(labels, (labels.shape[0], 1))\n",
    "# convert the data into float32 format\n",
    "features = features.astype(np.float32)\n",
    "labels = labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data set\n",
    "\n",
    "X_train = features[:features.shape[0]*8//10, :]\n",
    "X_test = features[features.shape[0]*8//10:, :]\n",
    "y_train = labels[:features.shape[0]*8//10, :]\n",
    "y_test = labels[features.shape[0]*8//10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the gradient descent \n",
    "# declaring the parameters and important variables\n",
    "no_of_data_points = X_train.shape[0]\n",
    "no_of_features = X_train.shape[1]\n",
    "no_of_epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights initialisation\n",
    "weights = np.random.normal(0, 1, (no_of_features, 1))\n",
    "# bias initialisation\n",
    "bias = np.random.normal(0, 1, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters for the gradient descent method\n",
    "learning_rate = 0.00000000001\n",
    "num_of_epochs = 100000\n",
    "derivative_weights = np.zeros((no_of_features, 1))\n",
    "derivative_intercept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No.: 0, MSE: 3392.2109375, Absolute Error: 52.189208984375\n",
      "Epoch No.: 10000, MSE: 2714.1455078125, Absolute Error: 45.9943962097168\n",
      "Epoch No.: 20000, MSE: 2248.6962890625, Absolute Error: 41.103965759277344\n",
      "Epoch No.: 30000, MSE: 1929.163330078125, Absolute Error: 37.34485626220703\n",
      "Epoch No.: 40000, MSE: 1709.7784423828125, Absolute Error: 34.66575241088867\n",
      "Epoch No.: 50000, MSE: 1559.184326171875, Absolute Error: 32.86613845825195\n",
      "Epoch No.: 60000, MSE: 1455.788818359375, Absolute Error: 31.83800506591797\n",
      "Epoch No.: 70000, MSE: 1384.7685546875, Absolute Error: 31.391223907470703\n",
      "Epoch No.: 80000, MSE: 1335.9691162109375, Absolute Error: 31.247987747192383\n",
      "Epoch No.: 90000, MSE: 1302.425537109375, Absolute Error: 31.164520263671875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_of_epochs):\n",
    "    # calculating the derivative of the weights and intercept\n",
    "    y_pred = np.dot(X_train, weights) + bias\n",
    "    difference = y_pred - y_train\n",
    "    derivative_weights = np.dot(X_train.T, difference)\n",
    "    derivative_intercept = np.sum(difference)\n",
    "    # updating the weights and intercept\n",
    "    weights = weights - learning_rate * derivative_weights\n",
    "    bias = bias - learning_rate * derivative_intercept\n",
    "    if (epoch%10000==0): \n",
    "        MSE = np.mean(np.square(y_train - y_pred))\n",
    "        print(f\"Epoch No.: {epoch}, MSE: {MSE}, Absolute Error: {np.mean(np.abs(y_train - y_pred))}\")\n",
    "        if (MSE < 0.0001):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1195.8896\n"
     ]
    }
   ],
   "source": [
    "# calculating the MSE for the testing data\n",
    "y_pred = np.dot(X_test, weights) + bias\n",
    "MSE = np.mean(np.square(y_test - y_pred))\n",
    "print(\"MSE: \", MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
