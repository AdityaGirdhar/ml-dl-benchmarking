{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages and libraries\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import np, npx\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "# npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IRIS dataset using pandas\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "y_labels = df.iloc[:, -1].values\n",
    "\n",
    "# encode the string class values as integers\n",
    "y_labels[y_labels == 'Iris-setosa'] = 0.0\n",
    "y_labels[y_labels == 'Iris-versicolor'] = 1.0\n",
    "y_labels[y_labels == 'Iris-virginica'] = 2.0\n",
    "\n",
    "# convert to mxnet.numpy.ndarray\n",
    "X = nd.array(X)\n",
    "y = nd.array(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train = X[:120]\n",
    "X_test = X[120:]\n",
    "y_train = y[:120]\n",
    "y_test = y[120:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data iterator\n",
    "batch_size = 10\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X_train, y_train), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = 3\n",
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights\n",
    "net.initialize(mx.init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Softmax Loss: 15.5255, Accuracy: 75.83333253860474%\n",
      "Epoch: 2, Softmax Loss: 1.3215, Accuracy: 80.83333373069763%\n",
      "Epoch: 3, Softmax Loss: 15.8061, Accuracy: 70.83333134651184%\n",
      "Epoch: 4, Softmax Loss: 4.5376, Accuracy: 58.33333134651184%\n",
      "Epoch: 5, Softmax Loss: 10.6335, Accuracy: 80.0000011920929%\n",
      "Epoch: 6, Softmax Loss: 1.8914, Accuracy: 71.66666388511658%\n",
      "Epoch: 7, Softmax Loss: 0.3778, Accuracy: 75.0%\n",
      "Epoch: 8, Softmax Loss: 41.0203, Accuracy: 74.16666746139526%\n",
      "Epoch: 9, Softmax Loss: 10.9027, Accuracy: 86.66666746139526%\n",
      "Epoch: 10, Softmax Loss: 13.0635, Accuracy: 49.166667461395264%\n",
      "Epoch: 11, Softmax Loss: 0.8404, Accuracy: 75.0%\n",
      "Epoch: 12, Softmax Loss: 1.3568, Accuracy: 58.33333134651184%\n",
      "Epoch: 13, Softmax Loss: 1.6326, Accuracy: 73.33333492279053%\n",
      "Epoch: 14, Softmax Loss: 0.0124, Accuracy: 83.33333134651184%\n",
      "Epoch: 15, Softmax Loss: 0.0028, Accuracy: 83.33333134651184%\n",
      "Epoch: 16, Softmax Loss: 6.4850, Accuracy: 83.33333134651184%\n",
      "Epoch: 17, Softmax Loss: 7.3223, Accuracy: 82.4999988079071%\n",
      "Epoch: 18, Softmax Loss: 19.3338, Accuracy: 58.33333134651184%\n",
      "Epoch: 19, Softmax Loss: 2.3087, Accuracy: 57.499998807907104%\n",
      "Epoch: 20, Softmax Loss: 1.5162, Accuracy: 58.33333134651184%\n",
      "Epoch: 21, Softmax Loss: 0.9905, Accuracy: 79.16666865348816%\n",
      "Epoch: 22, Softmax Loss: 0.0016, Accuracy: 83.33333134651184%\n",
      "Epoch: 23, Softmax Loss: 3.4892, Accuracy: 58.33333134651184%\n",
      "Epoch: 24, Softmax Loss: 0.4519, Accuracy: 59.16666388511658%\n",
      "Epoch: 25, Softmax Loss: 5.9084, Accuracy: 83.33333134651184%\n",
      "Epoch: 26, Softmax Loss: 0.0262, Accuracy: 83.33333134651184%\n",
      "Epoch: 27, Softmax Loss: 10.6416, Accuracy: 66.66666865348816%\n",
      "Epoch: 28, Softmax Loss: 0.7526, Accuracy: 83.33333134651184%\n",
      "Epoch: 29, Softmax Loss: 0.1013, Accuracy: 84.16666388511658%\n",
      "Epoch: 30, Softmax Loss: 5.2539, Accuracy: 60.83333492279053%\n",
      "Epoch: 31, Softmax Loss: 10.9713, Accuracy: 66.66666865348816%\n",
      "Epoch: 32, Softmax Loss: 0.4160, Accuracy: 80.0000011920929%\n",
      "Epoch: 33, Softmax Loss: 2.5415, Accuracy: 58.33333134651184%\n",
      "Epoch: 34, Softmax Loss: 3.8584, Accuracy: 79.16666865348816%\n",
      "Epoch: 35, Softmax Loss: 2.8431, Accuracy: 69.9999988079071%\n",
      "Epoch: 36, Softmax Loss: 5.2804, Accuracy: 60.83333492279053%\n",
      "Epoch: 37, Softmax Loss: 10.5571, Accuracy: 62.5%\n",
      "Epoch: 38, Softmax Loss: 2.7114, Accuracy: 55.000001192092896%\n",
      "Epoch: 39, Softmax Loss: 8.3081, Accuracy: 68.33333373069763%\n",
      "Epoch: 40, Softmax Loss: 0.3624, Accuracy: 83.33333134651184%\n",
      "Epoch: 41, Softmax Loss: 17.3072, Accuracy: 60.83333492279053%\n",
      "Epoch: 42, Softmax Loss: 0.8465, Accuracy: 83.33333134651184%\n",
      "Epoch: 43, Softmax Loss: 2.8362, Accuracy: 66.66666865348816%\n",
      "Epoch: 44, Softmax Loss: 12.3385, Accuracy: 85.00000238418579%\n",
      "Epoch: 45, Softmax Loss: 6.3723, Accuracy: 80.0000011920929%\n",
      "Epoch: 46, Softmax Loss: 5.9216, Accuracy: 58.33333134651184%\n",
      "Epoch: 47, Softmax Loss: 5.2479, Accuracy: 50.0%\n",
      "Epoch: 48, Softmax Loss: 0.0004, Accuracy: 75.0%\n",
      "Epoch: 49, Softmax Loss: 0.2856, Accuracy: 83.33333134651184%\n",
      "Epoch: 50, Softmax Loss: 9.5513, Accuracy: 83.33333134651184%\n",
      "Epoch: 51, Softmax Loss: 11.5148, Accuracy: 63.333332538604736%\n",
      "Epoch: 52, Softmax Loss: 7.2228, Accuracy: 75.0%\n",
      "Epoch: 53, Softmax Loss: 7.8900, Accuracy: 60.00000238418579%\n",
      "Epoch: 54, Softmax Loss: 0.0551, Accuracy: 85.00000238418579%\n",
      "Epoch: 55, Softmax Loss: 2.0987, Accuracy: 83.33333134651184%\n",
      "Epoch: 56, Softmax Loss: 2.4507, Accuracy: 71.66666388511658%\n",
      "Epoch: 57, Softmax Loss: 6.6693, Accuracy: 60.83333492279053%\n",
      "Epoch: 58, Softmax Loss: 2.4182, Accuracy: 68.33333373069763%\n",
      "Epoch: 59, Softmax Loss: 7.6793, Accuracy: 81.66666626930237%\n",
      "Epoch: 60, Softmax Loss: 9.4684, Accuracy: 68.33333373069763%\n",
      "Epoch: 61, Softmax Loss: 0.0009, Accuracy: 82.4999988079071%\n",
      "Epoch: 62, Softmax Loss: 9.9334, Accuracy: 82.4999988079071%\n",
      "Epoch: 63, Softmax Loss: 5.2259, Accuracy: 66.66666865348816%\n",
      "Epoch: 64, Softmax Loss: 7.9577, Accuracy: 69.16666626930237%\n",
      "Epoch: 65, Softmax Loss: 5.2431, Accuracy: 66.66666865348816%\n",
      "Epoch: 66, Softmax Loss: 0.0017, Accuracy: 76.66666507720947%\n",
      "Epoch: 67, Softmax Loss: 23.2312, Accuracy: 75.0%\n",
      "Epoch: 68, Softmax Loss: 0.3953, Accuracy: 76.66666507720947%\n",
      "Epoch: 69, Softmax Loss: 0.0607, Accuracy: 82.4999988079071%\n",
      "Epoch: 70, Softmax Loss: 9.5676, Accuracy: 83.33333134651184%\n",
      "Epoch: 71, Softmax Loss: 3.0471, Accuracy: 85.00000238418579%\n",
      "Epoch: 72, Softmax Loss: 4.3173, Accuracy: 64.99999761581421%\n",
      "Epoch: 73, Softmax Loss: 0.0001, Accuracy: 83.33333134651184%\n",
      "Epoch: 74, Softmax Loss: 11.9188, Accuracy: 73.33333492279053%\n",
      "Epoch: 75, Softmax Loss: 0.5648, Accuracy: 82.4999988079071%\n",
      "Epoch: 76, Softmax Loss: 4.5378, Accuracy: 55.83333373069763%\n",
      "Epoch: 77, Softmax Loss: 0.0728, Accuracy: 82.4999988079071%\n",
      "Epoch: 78, Softmax Loss: 0.9745, Accuracy: 82.4999988079071%\n",
      "Epoch: 79, Softmax Loss: 0.1460, Accuracy: 82.4999988079071%\n",
      "Epoch: 80, Softmax Loss: 12.0309, Accuracy: 69.9999988079071%\n",
      "Epoch: 81, Softmax Loss: 2.1630, Accuracy: 60.00000238418579%\n",
      "Epoch: 82, Softmax Loss: 8.6049, Accuracy: 75.83333253860474%\n",
      "Epoch: 83, Softmax Loss: 0.3980, Accuracy: 82.4999988079071%\n",
      "Epoch: 84, Softmax Loss: 7.9996, Accuracy: 60.83333492279053%\n",
      "Epoch: 85, Softmax Loss: 0.1718, Accuracy: 86.66666746139526%\n",
      "Epoch: 86, Softmax Loss: 0.9296, Accuracy: 81.66666626930237%\n",
      "Epoch: 87, Softmax Loss: 0.9464, Accuracy: 85.00000238418579%\n",
      "Epoch: 88, Softmax Loss: 4.3503, Accuracy: 58.33333134651184%\n",
      "Epoch: 89, Softmax Loss: 3.2372, Accuracy: 83.33333134651184%\n",
      "Epoch: 90, Softmax Loss: 1.0458, Accuracy: 86.66666746139526%\n",
      "Epoch: 91, Softmax Loss: 1.7576, Accuracy: 80.0000011920929%\n",
      "Epoch: 92, Softmax Loss: 12.0397, Accuracy: 59.16666388511658%\n",
      "Epoch: 93, Softmax Loss: 5.0420, Accuracy: 83.33333134651184%\n",
      "Epoch: 94, Softmax Loss: 3.7722, Accuracy: 86.66666746139526%\n",
      "Epoch: 95, Softmax Loss: 4.1353, Accuracy: 61.666667461395264%\n",
      "Epoch: 96, Softmax Loss: 8.8300, Accuracy: 58.33333134651184%\n",
      "Epoch: 97, Softmax Loss: 4.4635, Accuracy: 64.16666507720947%\n",
      "Epoch: 98, Softmax Loss: 0.1833, Accuracy: 83.33333134651184%\n",
      "Epoch: 99, Softmax Loss: 1.7596, Accuracy: 85.83333492279053%\n",
      "Epoch: 100, Softmax Loss: 2.1862, Accuracy: 84.16666388511658%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    cumulative_loss = 0  # loss for every epoch\n",
    "    for X_batch, y_batch in train_data:\n",
    "        with autograd.record():     # record the gradients\n",
    "            output = net(X_batch)   # forward pass\n",
    "            loss = loss_function(output, y_batch)   # calculate the loss\n",
    "        loss.backward()        # backward pass\n",
    "        optimizer.step(batch_size)   # updating the weights and bias\n",
    "        cumulative_loss += nd.mean(loss).asscalar()    # calculate the cumulative loss\n",
    "    accuracy = (net(X_train).argmax(axis=1) == y_train).mean().asscalar()\n",
    "    softmax_loss = nd.mean(loss).asscalar()\n",
    "    print(f\"Epoch: {epoch+1}, Softmax Loss: {softmax_loss:.4f}, Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# testing the model against the test dataset\n",
    "test_output = net(X_test)\n",
    "# predicted labels are the maximum value of the output along the axis 1\n",
    "predicted_labels = nd.argmax(test_output, axis=1)\n",
    "# calculate the accuracy of the model\n",
    "accuracy = (predicted_labels == y_test).mean().asscalar()\n",
    "print(f\"Test Accuracy: {accuracy * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
