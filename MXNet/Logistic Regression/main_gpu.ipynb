{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the libraries and packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages and libraries\n",
    "from mxnet import nd, gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet import profiler\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the context for the program \n",
    "mx.test_utils.list_gpus()\n",
    "if mx.context.num_gpus() > 0:\n",
    "    data_ctx = mx.gpu()\n",
    "    model_ctx = mx.gpu()\n",
    "else:\n",
    "    data_ctx = mx.cpu()\n",
    "    model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the profiler for measuring the execution time and memory usage\n",
    "profiler.set_config(profile_all=False,profile_symbolic = False, profile_imperative = False,profile_memory = True, profile_api = True,aggregate_stats=True,continuous_dump=False, filename='log_reg_sgd_profile.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and pre-processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "train_data = train_data.to_numpy()\n",
    "test_data = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = nd.array(train_data[:,1:], ctx=data_ctx)\n",
    "y_train = nd.array(train_data[:,0], ctx=data_ctx)\n",
    "X_test = nd.array(test_data[:,1:], ctx=data_ctx)\n",
    "y_test = nd.array(test_data[:,0], ctx=data_ctx)\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + nd.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "learning_rate = 0.1\n",
    "num_of_epochs = 10\n",
    "batch_size = 10\n",
    "weights = nd.random_normal(shape=(X_train.shape[1], 1), ctx=model_ctx)\n",
    "bias = 0\n",
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = gluon.data.ArrayDataset(X_train, y_train)\n",
    "data_loader = gluon.data.DataLoader(data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network\n",
    "net = nn.Sequential()\n",
    "# Adding the output layer with only one output, with the sigmoid activation function\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(units=1, activation='sigmoid'))\n",
    "# collect_params() will initialize the weights and biases for the neural network\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss function and the optimizer\n",
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(params = net.collect_params(), optimizer='sgd', optimizer_params={'learning_rate': learning_rate})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    cumulative_loss = 0\n",
    "    # for each epoch, iterating over the dataset in batches\n",
    "    for i, (data, label) in enumerate(data_loader):\n",
    "        # for the forward pass\n",
    "        with autograd.record():\n",
    "            output = net(data)      # output is the predicted value from the neural network\n",
    "            L = loss(output, label) # L will store the loss between the predicted value and the actual value\n",
    "        L.backward()                # for the backward pass\n",
    "        trainer.step(batch_size)    # updating the weights and biases\n",
    "        cumulative_loss += nd.sum(L).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running one epoch before profiling\n",
    "training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.nd.waitall() \n",
    "\n",
    "# starting the profiler\n",
    "start = time.time()\n",
    "profiler.set_state('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_of_epochs):\n",
    "    training_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for all operations to end, then stopping the profiler\n",
    "mx.nd.waitall()\n",
    "end = time.time()\n",
    "profiler.set_state('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = profiler.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results\n",
    "result = result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the result into a list of lists\n",
    "for i in range(len(result)):\n",
    "    result[i] = result[i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the maximum gpu and cpu memory usage and the total execution time\n",
    "max_gpu_use = 0\n",
    "max_cpu_use = 0\n",
    "total_execution_time = 0\n",
    "# traversing over the lists and trying to find the maximum gpu and cpu memory usage and the total execution time\n",
    "for i in result:\n",
    "    if (len(i)>=1 and i[0]=='Memory:'):\n",
    "        if (i[1]=='gpu/0'):\n",
    "            max_gpu_use = float(i[-2])\n",
    "        elif (i[1]=='cpu/0'):\n",
    "            max_cpu_use = float(i[-2])\n",
    "        else: continue\n",
    "    # if the length of the list 6 and the second to sixth elements are numbers, then it is a time entry\n",
    "    else:\n",
    "        if (len(i)>=6):\n",
    "            # if it is a valid time entry, then add it to the total execution time\n",
    "            if (re.match(r'^-?\\d+(?:\\.\\d+)$', i[-4]) is not None):\n",
    "                total_execution_time += float(i[-4])\n",
    "\n",
    "if (total_execution_time==0):\n",
    "    total_execution_time = (end - start)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum GPU memory usage: {max_gpu_use} KB\")\n",
    "print(f\"Maximum CPU memory usage: {max_cpu_use} KB\")\n",
    "print(f\"Total execution time: {total_execution_time} milli seconds (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reseting the profiler for the prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the profiler to default\n",
    "profiler.set_config(profile_all=False,profile_symbolic = False, profile_imperative = False,profile_memory = False, profile_api = True, aggregate_stats=True,continuous_dump=False, filename='log_reg_sgd_profile.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.nd.waitall() \n",
    "\n",
    "# starting the profiler\n",
    "profiler.set_state('run')\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the predicted values \n",
    "predicted_train = net(X_train)\n",
    "predicted_test = net(X_test)\n",
    "predicted_train[predicted_train >= 0.5] = 1\n",
    "predicted_train[predicted_train < 0.5] = 0\n",
    "predicted_test[predicted_test >= 0.5] = 1\n",
    "predicted_test[predicted_test < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for all operations to end, then stopping the profiler\n",
    "mx.nd.waitall()\n",
    "end = time.time()\n",
    "profiler.set_state('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the accuracy\n",
    "train_accuracy = nd.mean(predicted_train == y_train)\n",
    "test_accuracy = nd.mean(predicted_test == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = profiler.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results\n",
    "result = result.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the result into a list of lists\n",
    "for i in range(len(result)):\n",
    "    result[i] = result[i].split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the profiler data\n",
    "total_execution_time = 0\n",
    "for i in result:\n",
    "    if (len(i)>=6):\n",
    "        if (re.match(r'^-?\\d+(?:\\.\\d+)$', i[-4]) is not None):\n",
    "            total_execution_time += float(i[-4])\n",
    "\n",
    "if (total_execution_time==0):\n",
    "    total_execution_time = (end - start)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total prediction/testing time: {total_execution_time} milli seconds (ms)\")\n",
    "print(f\"Training accuracy: {train_accuracy.asscalar()*100}%\")\n",
    "print(f\"Testing accuracy: {test_accuracy.asscalar()*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
